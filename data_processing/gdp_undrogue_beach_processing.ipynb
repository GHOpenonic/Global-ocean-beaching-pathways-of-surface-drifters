{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2482085c-ff4f-4648-8e3c-bf9776c36b1c",
   "metadata": {},
   "source": [
    "### This code removes drogued drifters by temporally selecting drifter data after the date they lost their drogue. Next, it separates ultimately beaching drifters from never-beaching drifters and saves them to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0bd04fb-f4e1-4657-a3f3-2e6abf08cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "import os\n",
    "os.chdir('/dat1/openonic/Drifters') # directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b3d314-800e-4b79-b355-1fb27f040169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d40c560-af93-4bdf-b54c-b6cdb312e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "gdp = xr.open_dataset('gdp.nc', decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c29c26-7ebb-4339-9dad-19070af0a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# set up returned dataframe\n",
    "variables = ['id', 'time', 'lat', 'lon', 've', 'vn'] \n",
    "data_list = []\n",
    "\n",
    "# extract values\n",
    "ids = gdp.ids.values\n",
    "times = gdp.time.values\n",
    "lats = gdp.latitude.values\n",
    "lons = gdp.longitude.values\n",
    "ves = gdp.ve.values\n",
    "vns = gdp.vn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6092c2c0-7153-47f1-9c23-447eecfb3371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 498.8266108036041 seconds\n"
     ]
    }
   ],
   "source": [
    "# extract values and set to pandas dataframe\n",
    "start_time = time.time()\n",
    "data_list = [\n",
    "    {'id': i, 'time': time, 'lat': lat, 'lon': lon, 've': ve, 'vn': vn}\n",
    "    for i, time, lat, lon, ve, vn in zip(ids, times, lats, lons, ves, vns)\n",
    "]\n",
    "\n",
    "full_data = pd.DataFrame(data_list, columns=variables)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cd6d00-7234-492f-bb96-abd5423c42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_ids = []\n",
    "unbeach_ids = []\n",
    "\n",
    "for i in range(len(gdp.type_death)):\n",
    "    if gdp.type_death[i].item() == 1:\n",
    "        beach_ids.append(gdp.ID[i].item())\n",
    "    else:\n",
    "        unbeach_ids.append(gdp.ID[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a59abe3-70fe-4b53-92dd-9afe7778c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_full = full_data[full_data['id'].isin(beach_ids)] # get all ids that beached\n",
    "unbeach_full = full_data[full_data['id'].isin(unbeach_ids)] # get all ids that didn't beach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed621fe-d51b-474f-8d64-ddbc316320ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = gdp.drogue_lost_date.values # get time of drogue lost\n",
    "min_ID = gdp.drogue_lost_date.coords['ID'].values # get ids of drogue lost\n",
    "min_time_df = pd.DataFrame({'min_time': min_time, 'id': min_ID}) # put them in a DF together\n",
    "beach_min_time = min_time_df[min_time_df['id'].isin(beach_ids)] # separate into beached\n",
    "unbeach_min_time = min_time_df[min_time_df['id'].isin(unbeach_ids)] # and unbeached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748d65b9-7e57-4a17-b784-e8b2c0970693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beached\n",
    "merged_beach = pd.merge(beach_full, beach_min_time, left_on='id', right_on='id', how='inner')\n",
    "\n",
    "# remove drogued values using time of drogue loss\n",
    "undrogued_beach_ = merged_beach[merged_beach['time'] >= merged_beach['min_time']]\n",
    "\n",
    "# remove drogue loss time from df\n",
    "undrogued_beach = undrogued_beach_.drop(columns=['min_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2548f0fe-6cc5-4889-8bbe-c22bbd3c3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbeached\n",
    "merged_unbeach = pd.merge(unbeach_full, unbeach_min_time, left_on='id', right_on='id', how='inner')\n",
    "\n",
    "# remove drogued values using time of drogue loss\n",
    "undrogued_unbeach_ = merged_unbeach[merged_unbeach['time'] >= merged_unbeach['min_time']]\n",
    "\n",
    "# remove drogue loss time from df\n",
    "undrogued_unbeach = undrogued_unbeach_.drop(columns=['min_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34dd4b02-5e22-4eb1-aa9f-5e239996b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to undrogued_beach 'time to beach'\n",
    "\n",
    "\n",
    "# get the last time values of beached\n",
    "last_points = undrogued_beach.drop_duplicates(subset='id', keep='last')\n",
    "\n",
    "def find_time_to_beach(beach, beach_last,time_between_register_beaching_and_actually_beaching):\n",
    "    # DF to Numpy Arrays\n",
    "    beach_array = beach.to_numpy()\n",
    "    beach_last_array = beach_last.to_numpy()\n",
    "\n",
    "    # Create a dictionary to store the last known time for each ID\n",
    "    last_time_dict = dict(zip(beach_last_array[:, 0], beach_last_array[:, 1]))\n",
    "\n",
    "    beach_time = []\n",
    "\n",
    "    # Iterate through beach array and calculate time differences\n",
    "    for row in beach_array:\n",
    "        current_ID = row[0]\n",
    "        last_time = last_time_dict.get(current_ID, None)\n",
    "        \n",
    "        if last_time is not None:\n",
    "            current_time = row[1]\n",
    "            if current_time != last_time: # prevent 0 value --> infinite time as beach_last exists within beach\n",
    "                time_difference = last_time - current_time\n",
    "                beach_time.append(time_difference)\n",
    "            if current_time == last_time: # keep lat, lon, and values for hist shape the same\n",
    "                beach_time.append(time_between_register_beaching_and_actually_beaching)\n",
    "\n",
    "    return beach_time\n",
    "\n",
    "time_between_register_beaching_and_actually_beaching = 0 # immediate beaching (?)\n",
    "\n",
    "beach_time = find_time_to_beach(undrogued_beach, last_points, time_between_register_beaching_and_actually_beaching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcbe7be3-19f1-43f5-9064-9588848821e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "undrogued_beach['time_to_beach'] = beach_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973ffcdf-d90a-4263-a968-a3e3cdb4a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "undrogued_unbeach.to_csv('undrogued_unbeach.csv', index=False) # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9295ee7f-1719-469c-a774-61670dbe9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "undrogued_beach.to_csv('undrogued_beach.csv', index=False) # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca855b85-9721-454e-b0d7-eeb1d2af9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "894c13ac-a1d7-4062-a866-e5c3cc92487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.23282705 0.2009077  0.19944905]\n",
      "Sum of explained variance ratios: 0.6331838071078117\n"
     ]
    }
   ],
   "source": [
    "# unbeach\n",
    "pca_features = [undrogued_unbeach['lat'], undrogued_unbeach['lon'], undrogued_unbeach['time'], undrogued_unbeach['ve'], undrogued_unbeach['vn']]\n",
    "pca_df = pd.concat(pca_features, axis = 1)\n",
    "\n",
    "# PCA\n",
    "\n",
    "# Select the features for PCA\n",
    "features = ['lat', 'lon', 'time', 've', 'vn']\n",
    "\n",
    "# Extract the selected features from the concatenated dataframe\n",
    "X = pca_df[features]\n",
    "\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Replace infinite values with a large finite value or use a different strategy\n",
    "X.replace([np.inf, -np.inf],100000 , inplace=True)\n",
    "\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=3)  # You can choose the number of principal components\n",
    "principalComponents = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Creating a DataFrame with the principal components\n",
    "columns = [f'PC{i}' for i in range(1, pca.n_components_ + 1)]\n",
    "principalDf = pd.DataFrame(data=principalComponents, columns=columns)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio:\", explained_variance)\n",
    "print(\"Sum of explained variance ratios:\", np.sum(explained_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999d526-b649-46c5-8389-db348e49ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Calculating loadings\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# Creating a DataFrame with loadings\n",
    "loadings_df = pd.DataFrame(loadings, columns=columns, index=features)\n",
    "\n",
    "# Create a heatmap for the loadings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(loadings_df, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Loadings of Features on Principal Components: 5 PCs explain 93% of variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Original Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2376f5-f323-4eb5-8111-802e99e53085",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pca_df.corr()\n",
    "plt.figure(figsize=(24, 16))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 16})\n",
    "plt.title('Correlation Matrix',fontsize=30)\n",
    "#plt.savefig(path + 'Correlation Matrix.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dff8c4-70c1-4109-a3e1-bd16bd779534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
